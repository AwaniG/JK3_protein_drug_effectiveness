{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"17tIOy4DSbfj5z7dgt-yritPHRISQ70e-","timestamp":1698902198114},{"file_id":"1Iud18e0ct5gFBwxtOB45ES9ARCFk4Wtd","timestamp":1698901903194}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"xonBvq1DPJd7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"h3NnUBXeOnP5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install --pre deepchem\n","import deepchem\n","deepchem.__version__"],"metadata":{"id":"F74OkxNFOxqa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sys\n","!test -d bertviz_repo && echo \"FYI: bertviz_repo directory already exists, to pull latest version uncomment this line: !rm -r bertviz_repo\"\n","# !rm -r bertviz_repo # Uncomment if you need a clean pull from repo\n","!test -d bertviz_repo || git clone https://github.com/jessevig/bertviz bertviz_repo\n","if not 'bertviz_repo' in sys.path:\n","  sys.path += ['bertviz_repo']\n","!pip install regex"],"metadata":{"id":"DSykQCu7O4PP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","# Read the datasets\n","train = pd.read_csv('/content/drive/Shareddrives/1:1 Awani Gadre/Dataset/train.csv')\n","test = pd.read_csv('/content/drive/Shareddrives/1:1 Awani Gadre/Dataset/test.csv')"],"metadata":{"id":"QlwtcXgfPNY4"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OiZWWNPdOaMT"},"outputs":[],"source":["from transformers import AutoModelForMaskedLM, AutoTokenizer, RobertaModel, RobertaTokenizer\n","import pandas as pd\n","import torch\n","\n","X_train = train['canonical_smiles']\n","y_train = train['pIC50']\n","X_test = test['canonical_smiles']\n","y_test = test['pIC50']\n","\n","\n","# Function to get embeddings\n","def get_embeddings(data):\n","    token_ids = [tokenizer.encode(smile, add_special_tokens=True, max_length=512, truncation=True) for smile in data]\n","    embeddings = []\n","\n","    with torch.no_grad():\n","        for ids in token_ids:\n","            ids_tensor = torch.tensor(ids).unsqueeze(0)\n","            output = model(ids_tensor)[0]\n","            avg_embedding = output[0].mean(dim=0)\n","            embeddings.append(avg_embedding.numpy())\n","\n","    return embeddings\n","\n","# Load model and tokenizer\n","model = RobertaModel.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n","tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n","\n","# Get embeddings\n","train_embeddings = get_embeddings(X_train)\n","test_embeddings = get_embeddings(X_test)\n","\n","# Combine embeddings and descriptors\n","train_embed_df = pd.concat([pd.DataFrame(train_embeddings), y_train], axis=1)\n","test_embed_df = pd.concat([pd.DataFrame(test_embeddings), y_test], axis=1)\n","\n","\n","\n","# Optionally, save combined data to CSV\n","train_embed_df.to_csv('/content/drive/Shareddrives/1:1 Awani Gadre/Dataset/train_chemberta_descriptors_zinc.csv', index=False)\n","test_embed_df.to_csv('/content/drive/Shareddrives/1:1 Awani Gadre/Dataset/test_chemberta_descriptors_zinc.csv', index=False)"]}]}